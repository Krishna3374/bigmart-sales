{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# importing the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "u9q_uNtrwUGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to ignore warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# adjust the display settings so that the columns will not get truncated\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# reading the input\n",
        "train_data = pd.read_csv('train_data.csv')\n",
        "test_data = pd.read_csv('test_data.csv')\n",
        "\n",
        "# let's combine them as it is a hassle to apply every transformation twice\n",
        "train_data['source'] = 'train'\n",
        "test_data['source'] = 'test'"
      ],
      "metadata": {
        "id": "linubxX0Ozaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat([train_data, test_data], ignore_index=True)\n",
        "\n",
        "print(train_data.shape, test_data.shape, data.shape)\n",
        "\n",
        "# checking out missing values\n",
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "Ri5y22L8HUxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 5681 missing values in sales is because test data doesn't have it. So, it is fine.\n",
        "\n",
        "We need to fill in for item weight and outlet size."
      ],
      "metadata": {
        "id": "-ZK70E5MucMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check if there are any duplicate rows\n",
        "print(f'No. of rows with and without duplicates: {data.shape[0]}, {data.drop_duplicates().shape[0]}')"
      ],
      "metadata": {
        "id": "HzwmLyEniWSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data description\n",
        "data.describe(include='all')"
      ],
      "metadata": {
        "id": "TviMae_tu4IT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations**:\n",
        "\n",
        "Item and Outler identifier counts are as mentioned in the problem statement (1559, 10).\n",
        "\n",
        "Item type has 16 unique values. We may need to combine some to simplify things."
      ],
      "metadata": {
        "id": "oB0pTf-JyTH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check out the categorical values first\n",
        "col_name = 'Item_Fat_Content'\n",
        "data[col_name].value_counts()"
      ],
      "metadata": {
        "id": "u-ADww6tlJGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standardizing fat values\n",
        "data[col_name] = data[col_name].replace({'LF': 'Low Fat', 'low fat': 'Low Fat', 'reg': 'Regular'})\n",
        "data[col_name].value_counts()"
      ],
      "metadata": {
        "id": "IhhR63Ty0epr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Item_Type'].value_counts()"
      ],
      "metadata": {
        "id": "pjiWtgKJe_O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_id = 'Item_Identifier'\n",
        "data[item_id].head(50).unique()"
      ],
      "metadata": {
        "id": "j3yKWSY11h8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on values of item identifier, we can see a pattern of FDxxx, DRxxx, NCxxx.\n",
        "\n",
        "Here xxx is the id factor and first 2 letters being the broad category.\n",
        "\n",
        "The full form could be food (fruits, snacks, etc.), drinks (soft, hard) and non-consumables (household, health, etc.)."
      ],
      "metadata": {
        "id": "1SaJhDGc2I0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hence, making a new item category with these\n",
        "new_col = 'Item_Category'\n",
        "data[new_col] = data[item_id].apply(lambda x: x[:2])\n",
        "data[new_col].value_counts()"
      ],
      "metadata": {
        "id": "9s-_T-t32yfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a catch here. If there is a non consumable category, it doesn't make sense that the fat content column has only 2 unique values.\n",
        "\n",
        "It should be a data mistake and hence, let's make a separate fat category for this."
      ],
      "metadata": {
        "id": "OjF46A0H3ghX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# changing fat content to not applicable when item category is non consumable\n",
        "data.loc[data[new_col] == 'NC', 'Item_Fat_Content'] = 'NA'\n",
        "data['Item_Fat_Content'].value_counts()"
      ],
      "metadata": {
        "id": "mEh0U75S4hYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_name = 'Outlet_Size'\n",
        "data[col_name].value_counts(dropna=False)   # dropna false because we also have nulls here"
      ],
      "metadata": {
        "id": "6fWWCawn6Pz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filling in mode value based on outlet type\n",
        "type_size_mode = data.groupby('Outlet_Type')[col_name].transform(lambda x: x.mode()[0])\n",
        "data[col_name] = data[col_name].fillna(type_size_mode)\n",
        "data[col_name].value_counts(dropna=False)   # just to check if there are still nulls present"
      ],
      "metadata": {
        "id": "QupDr_aj7epv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, let's look at the numerical columns\n",
        "col_name = 'Item_Weight'\n",
        "print(data[col_name].min(), data[col_name].max())\n",
        "\n",
        "# filling up nulls with median weight based on item identifiers\n",
        "idt_weight_mean = data.groupby(item_id)[col_name].transform('mean')\n",
        "data[col_name] = data[col_name].fillna(idt_weight_mean)\n",
        "data[col_name].isnull().sum()   # checking if nulls are still present"
      ],
      "metadata": {
        "id": "L4TH71bUm5Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to turn off the legend globally\n",
        "plt.rc('legend', frameon=False)\n",
        "\n",
        "# let's check out the item weight distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 2))\n",
        "sns.histplot(data, x=col_name, ax=axes[0], kde=True)\n",
        "sns.boxplot(data, x=col_name, color='#ff7f50', ax=axes[1])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pJMq8GDvnEVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It doesn't seem like a normal distribution and there are no outliers."
      ],
      "metadata": {
        "id": "NWQJ-Fkbn6S2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_name = 'Item_Visibility'\n",
        "print(data[col_name].min(), data[col_name].max())\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 2))\n",
        "sns.histplot(data, x=col_name, color='#ff2052', ax=axes[0], kde=True)\n",
        "sns.boxplot(data, x=col_name, color='#915c83', ax=axes[1])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Cjhclfdcr1V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that it is right-skewed, which will have an impact on the models.\n",
        "Also, there are quite a lot of outliers."
      ],
      "metadata": {
        "id": "5QS0ws9Jtik5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# applying sqrt to reduce skewness and outliers\n",
        "new_col = 'Visibility_Sqrt'\n",
        "data[new_col] = np.sqrt(data[col_name])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 2))\n",
        "sns.histplot(data, x=new_col, color='#ff2052', ax=axes[0], kde=True)\n",
        "sns.boxplot(data, x=new_col, color='#915c83', ax=axes[1])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3BfuPBzxS_f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_name = 'Item_MRP'\n",
        "print(data[col_name].min(), data[col_name].max())\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 2))\n",
        "sns.histplot(data, x=col_name, color='#e5202a', ax=axes[0], kde=True)\n",
        "sns.boxplot(data, x=col_name, color='#760a13', ax=axes[1])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UJaB4r_Wq4EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No outliers and not a normal distribution."
      ],
      "metadata": {
        "id": "dym_nFalXvru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_name = 'Outlet_Establishment_Year'\n",
        "print(data[col_name].unique())\n",
        "\n",
        "# calculating outlet's age is more meaningful as it can help establish relation between age and sales effectively\n",
        "# the data is collected in 2013 according to the problem description, hence, using that here for age\n",
        "new_col = 'Outlet_Age'\n",
        "data[new_col] = 2013 - data[col_name]\n",
        "print(data[new_col].unique())"
      ],
      "metadata": {
        "id": "6HadkpprZUw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_name = 'Item_Outlet_Sales'\n",
        "print(data[col_name].min(), data[col_name].max())\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 2))\n",
        "sns.histplot(data, x=col_name, color='#c869f5', ax=axes[0], kde=True)\n",
        "sns.boxplot(data, x=col_name, color='#f6549c', ax=axes[1])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DY9OonV6WvGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target value is right skewed and has outliers."
      ],
      "metadata": {
        "id": "ZkiC57pbZRQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# applying sqrt to reduce skewness and outliers\n",
        "new_col = 'Sales_Sqrt'\n",
        "data[new_col] = np.sqrt(data[col_name])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 2))\n",
        "sns.histplot(data, x=new_col, color='#c869f5', ax=axes[0], kde=True)\n",
        "sns.boxplot(data, x=new_col, color='#f6549c', ax=axes[1])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z-Zlq7B2Z9Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping off unnecessary columns\n",
        "data.drop(columns=['Item_Visibility', 'Item_Type', 'Outlet_Establishment_Year',\n",
        "            'Item_Outlet_Sales'], inplace=True)\n",
        "\n",
        "target_col = 'Sales_Sqrt'\n",
        "data.columns"
      ],
      "metadata": {
        "id": "7m-iwfH_XjLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's make categorical columns numeric\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# we want original columns for submission\n",
        "le = LabelEncoder()\n",
        "data['Item_Id'] = le.fit_transform(data[item_id])   # variable item_id = Item_Identifier\n",
        "data['Outlet_Id'] = le.fit_transform(data['Outlet_Identifier'])\n",
        "\n",
        "cat_cols = ['Item_Fat_Content', 'Outlet_Size', 'Outlet_Location_Type',\n",
        "            'Outlet_Type', 'Item_Category']\n",
        "data = pd.get_dummies(data, columns=cat_cols, drop_first=True, dtype=np.int8)"
      ],
      "metadata": {
        "id": "i2pitaRZc1HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# separate train and test data based on source and dropping unnecessary columns\n",
        "source_col = 'source'\n",
        "train_data = data[data[source_col] == 'train'].drop(columns=[item_id, 'Outlet_Identifier', source_col])\n",
        "test_data = data[data[source_col] == 'test']\n",
        "\n",
        "res_data = test_data[[item_id, 'Outlet_Identifier']]\n",
        "test_data.drop(columns=[item_id, 'Outlet_Identifier', source_col, target_col], inplace=True)"
      ],
      "metadata": {
        "id": "ngDOHWF98hN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head(3)"
      ],
      "metadata": {
        "id": "v7V_EDmZbiM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head(3)"
      ],
      "metadata": {
        "id": "1r_cy0QtbiAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "# rmse is the evaluation metric as per the problem statement\n",
        "\n",
        "X = train_data.drop(target_col, axis=1)\n",
        "y = train_data[target_col]\n",
        "\n",
        "# splitting into training and validation data as there is already separate test data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "o8D8ijyscnk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {'LR': LinearRegression(), 'RR': Ridge(),\n",
        "          'RFR': RandomForestRegressor(random_state=42),\n",
        "          'GBR': GradientBoostingRegressor(random_state=42),\n",
        "          'LGBR': lgb.LGBMRegressor(verbose=-1, random_state=42)}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_val)\n",
        "    rmse = np.sqrt(mse(y_val, y_pred)).round(3)\n",
        "    print(f'Model: {name:<5}| RMSE: {rmse}')"
      ],
      "metadata": {
        "id": "vgsT8WtdOvy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting Regressor performed better than the rest."
      ],
      "metadata": {
        "id": "hnLjfO4a2poA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbr_model = models['GBR']\n",
        "gbr_model.fit(X_train, y_train)\n",
        "\n",
        "# print the feature importance in decending manner\n",
        "feat_imp = pd.Series(gbr_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "print(feat_imp)"
      ],
      "metadata": {
        "id": "gsfwQl0sYZAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using shap to check feature importance\n",
        "import shap\n",
        "\n",
        "explainer = shap.Explainer(gbr_model, X_train)\n",
        "shap_values = explainer(X_train)\n",
        "shap.summary_plot(shap_values, X_train, plot_type='bar')"
      ],
      "metadata": {
        "id": "mE5KLTN-UVcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validate to check the overall performance\n",
        "cv_model = GradientBoostingRegressor(random_state=42)\n",
        "scores = cross_val_score(cv_model, X_train, y_train, scoring='neg_root_mean_squared_error', cv=5)\n",
        "-scores.mean().round(3)"
      ],
      "metadata": {
        "id": "lOx1PKHMXtmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import StackingRegressor\n",
        "\n",
        "# taking the 3 best performed models above as base\n",
        "base_models = [('gb', GradientBoostingRegressor(random_state=42)), ('lr', LinearRegression()),\n",
        "    ('lgb', lgb.LGBMRegressor(verbose=-1, random_state=42))]\n",
        "meta_model = LinearRegression()\n",
        "\n",
        "sr = StackingRegressor(estimators=base_models, final_estimator=meta_model)\n",
        "sr.fit(X_train, y_train)\n",
        "y_pred = sr.predict(X_val)\n",
        "rmse = np.sqrt(mse(y_val, y_pred)).round(3)\n",
        "print('RMSE:', rmse)"
      ],
      "metadata": {
        "id": "SKqoqFaajzi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's train the best model on the validation data\n",
        "# and then predict the test data to save the results\n",
        "sr.fit(X_val, y_val)\n",
        "y_pred = sr.predict(test_data)\n",
        "\n",
        "res_data['Item_Outlet_Sales'] = np.square(y_pred)\n",
        "res_data.to_csv('results.csv', index=False)"
      ],
      "metadata": {
        "id": "cKvojx9Sigv7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}